[
["index.html", "Data Science for Policy Analysts Chapter 1 Prerequisites", " Data Science for Policy Analysts Chris Lavoie 2019-07-03 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 2 Introduction 2.1 The Policy Analyst’s Challenge 2.2 The Data Scientist’s Challenge 2.3 Information Asymmetry 2.4 Goals", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. 2.1 The Policy Analyst’s Challenge Data Scientists have an interesting position in policy circles these days. They have been coming into meeting rooms as evangelists for an innovative new way of approaching age-old problems that have vexed policy makers for decades. They are able to enter the room with their heads held high, MacBooks tucked gently under their arms as they assume the head of the table. They wield a new vocabulary and confident promises that they can revolutionize whatever policy problem you’re having with just a few simple scripts. These data scientists have command over a poweful language of technical terms that have been propelled to hypnotic power thanks to sensational articles from business magazines. Companies like the Economist and Harvard Business Review have formed a symbiotic relationship with data scientists – The latter shows a proof of concept of an interesting new idea, and the former will propel the idea to the stratosphere with their articles asking if a new technological world order is upon us. Data scientists use this new language, along with a screen with several command lines open on one half of the screen, and long files of code on the other before they open the sleek new interface to a product that will solve your problem. Like a wizard uttering a spell, they use technical terms like artifical intelligence, gradient descent or algorithm to assert their authority of the room. All the “non-technical” folks in the room are immediately subject to a sinister form of information asymmetry. The articles that appear in their inboxes tell them that these new technologies are the key to staying ahead in the new world, and the data scientist in front of them are providing an example of this right in front of them. The example they see is compelling, and if the data scientist’s claims are true, how could they refuse them? The data scientist has a clear advantage in these situations. Policy analysts are often desperate to learn about this new environment that is being thrust upon them. The articles being written in every business magazine are telling them that they will be left behind if they don’t adopt these technologies. Unfortunately for these analysts, they don’t have the time or the resources to even learn about the techniques that data scientists champion in their meetings. Their arms are twisted in these meetings. They can’t question the techniques and assumptions that data scientists are employing before their eyes, and they can’t refuse to give these methods a shot for fear of ridicule from management above and below. If they reject the data scientist, they are often mocked as a dinosaur, and are warned that they are holding the organization back from modernization. The data scientist is now ready to go in for the kill. The policy analysts are powerless, and are all but forced to accept the data scientist’s proposal. The data scientist asks for a contract to continue working on this project. They ask for more data, more resources, and time. The policy analyst is in no position to refuse, and accepts. Perhaps the manager and analyst walking out of the meeting are excited for what’s coming next. Or maybe they’re confused, skeptical and disoriented. To the data scientist, it almost doesn’t matter. They walked into that meeting room with a game plan to wrestle the analysts into submission, and it worked without a hitch, as it almost often does. I’ve seen this story unfold in front of me countless times. I’ve seen managers in the policy space rubbing their head trying to understand why these methods are starting to work. When the analysts poke holes in the demonstration, data scientists are quick to remind them that the method they’re proposing is a proof of concept, and not nearly completed. This neutralizes the analysts, leading them right into the palm of the data scientist’s hands. 2.2 The Data Scientist’s Challenge The story is a little different from the data scientist’s perspective. They have seen an institutional problem, and they see a pool of data that is not being utilized by the policy analysts. They’ve done the research in the techniques and methods that could address this problem. They know that there is an open source package that can implement the statistical techniques necessary to help this problem, and they know how to get a hold of the data. They get to work quickly, excited about the impact that their technology can have. After a week or two of sprinting on this problem, they finish their proof of concept and quickly arrange a meeting to show the exciting new technology they have. The data scientist prepares their presentation. They drum up a couple of quick slides to explain what they see the problem is, and what the technology they’ve developed can do to solve the problem. They purposely omit the technical mathematical concepts, replacing it with intuitive explanations of the technology–not because they want to hide behind these techniques, but because they want to focus on the results. The mechanics behind technology can be intimidating and would distract from what’s actually going on. While they would love to talk about the methods and intuition behind the mathematics that makes their new technology possible, but they have so much to cover and so little time to do so. The data scientist believes in their methods, and knows that they can do great things with just a little bit more time, and a little more data. Of course the policy analyst is going to find errors in the technology, the data scientist only had two weeks to build this proof of concept! The algorithms behind the technology could be so much more effective with a little tweaking, and with more descriptive data. There are going to be errors in the assumptions, because the data scientist didn’t access to the expertise that the policy analysts have spent years accumulating. They are trying to communicate what the technology can do now, and what it could do in the future–if only they could get the managers to sign on and collaborate in the development. 2.3 Information Asymmetry The policy analysts and managers in our meeting are forced to deal with a troubling disadvantage. They have to decode the all the information presented in a different language to them. They have to think critically about the implications of the technique they likely have never seen, and make a call on whether to pursue this project. This is an unfortunate case of asymmetric information on both sides. The data scientists want to communicate the work they’re doing, but they can only guess as to the exact needs of the client. The analysts themselves are caught unaware of the mechanics behind the technology they’re being pressured to endorse. The asymmetry could lead to the two parties talking past each other, not really addressing the other’s questions. At worst, loads of time and money can be spent developing a product that doesn’t quite fit the needs of the client. It’s a tricky situation to navigate, often requiring one of the party’s to understand the exact situation of the other and translate their methods to the other’s language. This communication is a challenging art that comes with experience. For policy analysts, an understanding of the techniques used in a data scientist’s toolkit can help bridge the communication gap between the two. In more sinister cases, a knowledge of the intuition behind data science’s most sophisticated techniques could prove most useful as a defense from clueless colleagues or nefarious consultants. 2.4 Goals The goal of this book is to introduce some of the most important concepts in data science to a non-technical policy analyst. Each chapter will present a different set of concepts to the reader, explaining the intuition and ideas behind some of the most popular technology in the world. The hope is that the reader will understand the concepts in data science well enough to identify the main ideas behind the technology they see every day. They will have better judgement in implementing data science in their own projects, and will be able to communicate better with data scientists. I find the ideas behind data science to be beautiful. There is elegant simplicity in some of data science’s most popular workflows. I believe these ideas can stand on their own, and can be communicated without technical background. You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["whatis.html", "Chapter 3 What is a Data Scientist? 3.1 Data Manipulation and Programming 3.2 Statistical Knowledge 3.3 Solving Business Problems 3.4 Code", " Chapter 3 What is a Data Scientist? One of the first questions that might have occurred to you as you’ve heard about data scientists is: What do they do? This is actually not quite a simple answer. Data science has come to serve as a label for data specialists. They are able to approach a wide swath of problems due to their ability to work in data in all of its forms. Data scientists are master data manipulators. They are often well-versed in statistics, and they thrive on solving business problems with their skills. 3.1 Data Manipulation and Programming This expertise in data manipulation comes thanks to the toolkit at their disposal. Rather than relying on old proprietary software, or be limited by a graphical interface, data scientists almost always use programming languages to manipulate data. Because of the power and flexibility of these programming languages like Python and R, they’re able to work with gigabytes of data with millions of rows as easily as they would work with ten rows. They aren’t daunted by size. In fact, most data scientists I’ve met salivate over massive datasets, because more data gives them many more possibilities. These programming languages are specialized to move data into and out of different environments and formats. A data scientist can pull data from the a hundred websites, combine the relevant information into one dataset and perform statistical analysis from there. Often, they spend a lot of time cleaning the dataset. They purge it of impurities, impute missing values and convert messy data into a beautiful tabular dataset. In fact, the saying goes that eighty percent of a data scientist’s time is spent cleaning data. Below is an example of a function that a data scientist might employ in order to manipulate the data, using the programming language Python. The particular function expands a dataset to include exponential transformations of its variables. It’s an example of code that would be used to rapidly generate lots of different data from an existing dataset. def df_expansion(data_frame, order=2, interactions = False): # Creates empty dataframe # We will append our new columns into the df, and then # concatenate our new frame to the original new_df = pd.DataFrame() relevant_col_list = [] def check_if_relevant(df): for col in df.columns: unique_vals = list(df[col].unique()) if not (len(unique_vals) == 2 and 0 in unique_vals and 1 in unique_vals): relevant_col_list.append(col) check_if_relevant(data_frame) for exponent in (range(1,order)): exponent += 1 # For each exponent, add columns for col in relevant_col_list: new_df[col+&#39;**&#39;+str(exponent)] = data_frame[col]**exponent if interactions: for col1 in data_frame.columns: for col2 in data_frame.columns: if col1 != col2: new_df[col1+&quot;*&quot;+col2] = data_frame[col1]*data_frame[col2] new_dataframe = pd.concat([data_frame, new_df], axis=1) return new_dataframe Using code like the example above, they are able to rapidly evaluate large amounts of data in order to solve the problem at hand. Data that becomes difficult to manage in Excel is mincemeat for a data scientist. A typical day for a data scientist will involve an immense amount of data cleaning, and these skills should not be discounted. 3.2 Statistical Knowledge The common saying goes that data scientists know more computer science than a statistician, and more statistics than a computer scientist. While I’m not convinced this is very true, it does provide a useful heuristic to think about data scientists. Their ability to move data from A to B is an invaluable skill, however they often rely on statistical analysis and data visualization to extract usefulness from the data. This is where the reported sexiness of a data scientist comes into play [CITATION]. With a deep knowledge of statistics, a data scientist is able to tease out patterns and relationships from the data they are analyzing. Some of the most well-known tools in their belt include Machine Learning, Natural Language Processing, A/B Testing, among many others. This is probably the most well-known part of the data scientist’s skillset and the statistical concepts and ideas they employ will be the subject of most of this book. 3.3 Solving Business Problems A data scientist is not a theorist. They are practicioners with a large toolkit, looking to solve practical problems in the real world. 3.4 Code As I mentioned before, data scientists often solve their data problems using code. The advantages of doing so are numerous. Writing code allows the analyst to be incredible specific about what they want the computer to do for them. They can work with large amounts of data with the same ease as smaller tables. But by far, the biggest advantage of working with code is the transferability: At the end of the day. The code that an individual writes is just text. It’s merely a string of characters put together that, when put through the write interpreter, can command the computer to do certain specific things. We can easily download the same interpreter, and the code that you write should be just as good on my machine. The ideas behind code are often incredibly powerful. Because of this, software was often kept private. While the format of the code is just text, the instructions that the machine executes can be invaluable, and are economically essential for a company to keep secret, lest they lose all exclusivity over the product. But this is not the case for the Free and Open Source Movement. In fact, in the 1950’s and 1960’s, software was primarily written by academics who shared their code freely and openly. It wasn’t until the 1970’s and 1980’s that software was generally closed off from the public. Programming languages and their compilers are generally released as open-source software. Python and R are no exceptions. As mentioned before, Python and R are two of the most common programming languages for data science. A key step in the rise of these languages in data science comes from the fact that they are free and open-source. They are freely available for anyone to use and write code in. When statisticians (who flocked primarily to the R programming language) started writing their own packages–code wrapped into a bundle so as to be distributed–they shared them openly online. Their colleagues, or anyone who stumbled upon them, could use the code written and scrutinized by professionals in their field. Over the years, the number of statistical packages on both Python and R have grown tremendously. Data scientists today have access to state of the art methods of data and statistical analysis, and they’re not afraid to use them. Data scientists are able to reach for open-source software implementations of some of the most powerful techniques around today and implement them in almost no time. Their analysis stands on the shoulders of benevolent giants, to the benefit of everyone. https://en.wikipedia.org/wiki/History_of_free_and_open-source_software https://en.wikipedia.org/wiki/Open-source-software_movement https://journal.r-project.org/archive/2009-1/RJournal_2009-1_Chambers.pdf "],
["final-words.html", "Chapter 4 Final Words", " Chapter 4 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
